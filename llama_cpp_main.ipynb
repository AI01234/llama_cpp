{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "61e6ea6a-2b36-423e-ae31-4e4094c67126",
   "metadata": {},
   "source": [
    "This file is made to simplify the file that I read for my project.\n",
    "\n",
    "I think there would be some missing points because I write this file just about what I needed to read. \n",
    "\n",
    "If you want to read the original file, \n",
    "please visit this link:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5053b038-1050-47db-942b-01013d737456",
   "metadata": {},
   "source": [
    "https://github.com/ggerganov/llama.cpp/blob/master/examples/main/README.md#input-prompts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b067a6a-33ec-4999-939f-13d8d5afcafb",
   "metadata": {},
   "source": [
    "# llama.cpp/example/main"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8746f179-f6ae-43c7-bf3d-34ba17c2da28",
   "metadata": {},
   "source": [
    "이 예제 프로그램을 사용하면 다양한 LLaMA 언어 모델을 쉽고 효율적으로 사용할 수 있습니다.\n",
    "\n",
    "이는 더 빠르고 더 낮은 메모리를 위한 선택적 4비트 양자화 지원과 함께 일반 C/C++ 구현을 제공하는 [llama.cpp](https://github.com/ggerganov/llama.cpp) 프로젝트와 함께 작동하도록 특별히 설계되었습니다. \n",
    "\n",
    "추론이며 데스크톱 CPU에 최적화되어 있습니다. 이 프로그램은 사용자가 제공한 프롬프트를 기반으로 텍스트를 생성하고 역방향 프롬프트를 사용한 채팅과 같은 상호 작용을 포함하여 LLaMA 모델을 사용하여 다양한 추론 작업을 수행하는 데 사용할 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a809d03-e11a-409a-bb91-59268c0f96ce",
   "metadata": {},
   "source": [
    "## Table of Contents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c61c6d7-a602-4931-890e-945f5e0a3eb2",
   "metadata": {},
   "source": [
    "1. [Quick Start](#quick-start)\n",
    "2. [Common Options](#common-options)\n",
    "3. [Input Prompts](#input-prompts)\n",
    "4. [Interaction](#interaction)\n",
    "5. [Context Management](#context-management)\n",
    "6. [Generation Flags](#generation-flags)\n",
    "7. [Performance Tuning and Memory Options](#performance-tuning-and-memory-options)\n",
    "8. [Additional Options](#additional-options)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ed74b0f-b9a1-4479-9ed3-40d6c4df45dd",
   "metadata": {},
   "source": [
    "## Quick Start\n",
    "\n",
    "To get started right away, run the following command, making sure to use the correct path for the model you have:\n",
    "\n",
    "<<Please go to the original file if you needed to do this option.(quick start)>>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a5862a8-6184-4451-96a8-2b1b0b5c62c8",
   "metadata": {},
   "source": [
    "## Common Options\n",
    "\n",
    "가장 기본으로 사용되는 옵션들에 대해서 다루게 됩니다. \n",
    "\n",
    "-   `-m FNAME, --model FNAME`: 모델의 파일 경로 저장(e.g., `models/7B/ggml-model.bin`).\n",
    "-   `-i, --interactive`: 대화형 모드, 실시간 응답\n",
    "-   `-ins, --instruct`: 명령 모드에서 프로그램을 실행, Alpaca 모델 작업 시 유용.\n",
    "-   `-n N, --n-predict N`: 텍스트 생성 시 예측할 토큰 수를 설정\n",
    "-   `-c N, --ctx-size N`:프롬프트 컨텍스트의 크기를 설정. 기본값은 512이지만 LLaMA 모델은 2048의 컨텍스트로 구축되었으므로 더 긴 입력/추론에 대해 더 나은 결과를 제공합니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc23495f-483b-4a61-9aea-6a699a1fcd98",
   "metadata": {},
   "source": [
    "## Input Prompts\n",
    "\n",
    "The `main` program provides several ways to interact with the LLaMA models using input prompts:\n",
    "'main' 프로그램은 입력 프롬프트를 사용하여 LLaMA 모델과 상호 작용하는 여러 가지 방법을 제공합니다.\n",
    "\n",
    "-   `--prompt PROMPT`: Provide a prompt directly as a command-line option.( 명령중 옵션으로 프롬프트 제공)\n",
    "-   `--file FNAME`: Provide a file containing a prompt or multiple prompts. (하나 또는 여래개의 프롬프트가 포함된 파일 제공)\n",
    "-   `--interactive-first`: Run the program in interactive mode and wait for input right away. (More on this below.) (프로그램을 대화형 모드로 실행하고 바로 입력을 기다리기)\n",
    "-   `--random-prompt`: Start with a randomized prompt.(무작위 프롬프트 시작)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fb7a807-18f4-440e-bf57-a80e73fbf11f",
   "metadata": {},
   "source": [
    "## Interaction\n",
    "\n",
    "\n",
    "'메인' 프로그램은 LLaMA 모델과 원활하게 상호 작용할 수 있는 방법을 제공하여 사용자가 실시간 대화에 참여하거나 \n",
    "\n",
    "특정 작업에 대한 지침을 제공할 수 있도록 합니다. \n",
    "\n",
    "대화형 모드는 `--interactive`, `--interactive-first` 및 `--instruct`를 포함한 다양한 옵션을 사용할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aaa4865-eddd-4aff-ae0d-0241511c9d4c",
   "metadata": {},
   "source": [
    "대화형 모드에서 사용자는 프로세스 진행중에 원하는 때에 입력을 진행할 수 있다. `Ctrl+C` 옵션을 이용하여 `Return` 가능\n",
    "\n",
    "입력을 완료하지 않고 추가 줄을 제출하려면 (`\\`) 로 끝내고 계속해서 입력을 진행하면 된다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90cae5e3-e380-41f5-baa4-f59953a43e30",
   "metadata": {},
   "source": [
    "### Interaction Options"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56450cd5-1fe2-4557-a068-bb2da6d75540",
   "metadata": {},
   "source": [
    "-   `-i, --interactive`:  실시간 대화 모드\n",
    "-   `--interactive-first`: 대화형 모드에서 프로그램을 실행하고 텍스트 생성을 시작하기 전에 즉시 사용자 입력을 기다리기\n",
    "-   `-ins, --instruct`: 명령어 모드에서 작동, Alpaca 모델과 함께 작동하도록 특별히 설계된 지침모드에서 프로그램 실행\n",
    "-   `--color`: 프롬프트, 사용자 입력 및 생성된 텍스트를 시각적으로 구별하기 위해 색상화된 출력을 활성화\n",
    "\n",
    "이러한 상호 작용 옵션을 이해하고 활용하면 LLaMA 모델을 사용하여 흥미롭고 역동적인 경험을 만들어 특정 요구 사항에 맞게 텍스트 생성 프로세스를 조정할 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fb31988-6074-4330-a4e7-2f6ec2f93dba",
   "metadata": {},
   "source": [
    "### Reverse Prompts\n",
    "\n",
    "Reverse prompts are a powerful way to create a chat-like experience with a LLaMA model by pausing the text generation when specific text strings are encountered:\n",
    "\n",
    "-   `-r PROMPT, --reverse-prompt PROMPT`: Specify one or multiple reverse prompts to pause text generation and switch to interactive mode. For example, `-r \"User:\"` can be used to jump back into the conversation whenever it's the user's turn to speak. This helps create a more interactive and conversational experience. However, the reverse prompt doesn't work when it ends with a space.\n",
    "\n",
    "To overcome this limitation, you can use the `--in-prefix` flag to add a space or any other characters after the reverse prompt."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd2a16cb-6d7f-4d2f-99e6-385dbcd038d1",
   "metadata": {},
   "source": [
    "역방향 프롬프트는 텍스트 생성을 일시 중지하고 대화형 모드로 전환하여 사용자가 특정 텍스트 문자열을 발견할 때마다 대화를 진행할 수 있도록 하는 기능입니다. 예를 들어, 사용자가 \"User:\"라는 문자열을 입력할 때마다 대화가 시작되고, 모델은 이를 인식하여 사용자와 상호작용하도록 됩니다.\r\n",
    "\r\n",
    "이 기능은 사용자가 텍스트 생성 과정을 제어하고, 모델이 특정 단어나 문구를 발견할 때마다 대화를 진행하도록 할 수 있습니다. 이를 통해 보다 상호작용적이고 대화적인 경험을 만들 수 있습니다.\r\n",
    "\r\n",
    "그러나 역방향 프롬프트가 작동하기 위해서는 역방향 프롬프트 문자열 뒤에 공백이 오면 안 됩니다. 이를 극복하  위해 --in-prefix 플래그를 사용하여 역방향 프롬프트 뒤에 공백이나 다른 문자를 추가할 수 있습니다.\r\n",
    "\r\n",
    "즉, 이 기능은 사용자가 특정한 단어나 문자열을 입력하면 모델이 해당 입력을 인식하여 대화를 시작하고, 이를 통해 사용자와 모델 간의 상호작용을 더욱 증진시킬 수 있는 방법을 제공합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df0fcc17-0337-45fe-8285-be147cac7c3f",
   "metadata": {},
   "source": [
    "### In-Prefix(접두사)\n",
    "\n",
    "The `--in-prefix` flag is used to add a prefix to your input, primarily, this is used to insert a space after the reverse prompt. Here's an example of how to use the `--in-prefix` flag in conjunction with the `--reverse-prompt` flag:\n",
    "\n",
    "```sh\n",
    "./main -r \"User:\" --in-prefix \" \"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8e935df-595a-459f-8f6d-856533950b6c",
   "metadata": {},
   "source": [
    "### In-Suffix(접미사)\n",
    "\n",
    "The `--in-suffix` flag is used to add a suffix after your input. This is useful for adding an \"Assistant:\" prompt after the user's input. It's added after the new-line character (`\\n`) that's automatically added to the end of the user's input. Here's an example of how to use the `--in-suffix` flag in conjunction with the `--reverse-prompt` flag:\n",
    "\n",
    "```sh\n",
    "./main -r \"User:\" --in-prefix \" \" --in-suffix \"Assistant:\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d5df604-d3d1-4226-9780-f5fe3d0313c0",
   "metadata": {},
   "source": [
    "## Context Management( context 크기 제한)\n",
    "\n",
    "During text generation, LLaMA models have a limited context size, which means they can only consider a certain number of tokens from the input and generated text. When the context fills up, the model resets internally, potentially losing some information from the beginning of the conversation or instructions. Context management options help maintain continuity and coherence in these situations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05b0823b-78db-449f-8b2e-ecd0a93d9530",
   "metadata": {},
   "source": [
    "`-c N, --ctx-size N`: 최대 2048 기준, 큰 context에서 성능 좋다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61325e69-1e94-47ff-96a6-1678d3ba50f7",
   "metadata": {},
   "source": [
    "### Extended Context Size\n",
    "\n",
    " 효율적인 연산을 통해 긴 시퀀스에 대한 어텐션을 수행할 수 있도록 하는 것을 의미. \n",
    "\n",
    " 예를 들어, 사전 훈련된 컨텍스트 길이가 4k이고 미세 조정된 모델의 컨택스트 길이가 32k인 경우에 8의 배수이므로\n",
    "\n",
    " `--rope-scale`을 8로 설정하여 작동이 가능하다. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67e0ec96-2300-49e5-b165-7169f0774de6",
   "metadata": {},
   "source": [
    "### Keep Prompt\n",
    "\n",
    "`--keep`: 모델에서 context 크기가 부족하여 원래 대화 주제는 유지하기 위해서 활용한다.\n",
    "`--keep N`: 모델이 내부 컨텍스트를 재설정할 때 유지할 초기 프롬프트의 토큰 수를 지정(기본적으로 0사용, 모든 토큰을 유지하기 위해서는 -1이용)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7533725e-1c5e-431f-9762-c1478baa0073",
   "metadata": {},
   "source": [
    "## Generation Flags\n",
    "\n",
    "텍스트에 대한 여러 옵션들을 바꿀 때 사용하는 Flag를 의미한다. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "614861cd-f788-4266-8dc3-a9eb1372b44d",
   "metadata": {},
   "source": [
    "### Number of Tokens to Predict\n",
    "\n",
    "-   `-n N, --n-predict N`: 텍스트 생성 시 예측 할 토큰 수 (default: 128, -1 = infinity, -2 = until context filled)\n",
    "\n",
    "The `--n-predict` : 입력 프롬프트에 대한 응답으로 모델이 생성하는 토큰 수를 제어.\n",
    "\n",
    "값이 -1인 경우에는 무한한 텍스트 생성이 가능. \n",
    "\n",
    "EOS(End-of-Sequence) 토큰 또는 역 프롬프트가 발생하는 경우 생성된 텍스트가 지정된 토큰 수보다 짧을 수 있다는 점에 유의하는 것이 중요합니다. 대화형 모드에서는 텍스트 생성이 일시 중지되고 제어권이 사용자에게 반환됩니다. 비대화형 모드에서는 프로그램이 종료됩니다. 두 경우 모두 지정된 `n-predict` 값에 도달하기 전에 텍스트 생성이 중지될 수 있습니다. 모델이 자체적으로 시퀀스 종료를 생성하지 않고 계속 진행되도록 하려면 `--ignore-eos` 매개변수를 사용할 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "087e2b19-8913-4601-803f-dc98885c5238",
   "metadata": {},
   "source": [
    "### Temperature\n",
    "\n",
    "-   `--temp N`: 생성된 텍스트의 무작위성 조정하는 하이퍼파라미터 (default: 0.8).\n",
    "\n",
    "온도가 높을수록 무작위성이 높아진다. 극단적으로 온도가 0이면 항상 가능성이 가장 높은 다음 토큰을 선택하여 각 실행에서 동일한 출력이 발생한다.\n",
    "\n",
    "Example usage: `--temp 0.5`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "375f7d60-b436-491f-8b55-389286ef01c3",
   "metadata": {},
   "source": [
    "### Repeat Penalty\n",
    "\n",
    "-   `--repeat-penalty N`: 생성된 텍스트에서 토큰 시퀀스의 반복을 제어 (default: 1.1)., 단조로운 텍스트 생성 방지\n",
    "-   `--repeat-last-n N`: 반복에 대한 처벌을 고려할 마지막 n 토큰 (default: 64, 0 = disabled, -1 = ctx-size).\n",
    "-   `--no-penalize-nl`:반복 페널티를 적용할 때 개행 토큰에 대한 페널티를 비활성화\n",
    "\n",
    "`--no-penalize-nl`: 줄 바꿈 패널티 비활성화에 이용\n",
    "\n",
    "Example usage: `--repeat-penalty 1.15 --repeat-last-n 128 --no-penalize-nl`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "582c92e6-4776-43d2-9344-9cc9bf5aaf6d",
   "metadata": {},
   "source": [
    "### Top-K Sampling\n",
    "\n",
    "-   `--top-k N`: 가능성이 높은 N개의 토큰 뽑기(default:40)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c1207ef-78e9-4af7-a494-86a79d96ccc7",
   "metadata": {},
   "source": [
    "### Top-P Sampling\n",
    "\n",
    "-   `--top-p N`: 다음 토큰 선택을 임계값 P(기본값: 0.9)보다 높은 누적 확률을 가진 토큰의 하위 집합으로 제한\n",
    "\n",
    "Example usage: `--top-p 0.95`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7bef6c5-4db8-431b-b3e8-5795d6a24c65",
   "metadata": {},
   "source": [
    "### Min P Sampling\n",
    "\n",
    "-   `--min-p N`: 토큰 선택을 위한 최소 기본 확률 임계값 설정(default:0.05)\n",
    "\n",
    "Example usage: `--min-p 0.05`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "727925f8-b091-4c60-9152-49a83b333334",
   "metadata": {},
   "source": [
    "\n",
    "### Tail Free Sampling (TFS)\n",
    "\n",
    "-   `--tfs N`: tail free sampling을 z를 이용해 가능하게 함 (default: 1.0, 1.0 = disabled).\n",
    "\n",
    "TFS(Tail Free Sampling)는 가능성이 낮은 토큰(관련성이 낮거나 일관성이 없거나 무의미할 수 있음)이 출력에 미치는 영향을 줄이는 것을 목표로 하는 텍스트 생성 기술입니다.\n",
    "\n",
    "Example usage: `--tfs 0.95`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5595b1c2-b8c7-44e5-835a-e97ff3b497f6",
   "metadata": {},
   "source": [
    "### Locally Typical Sampling\n",
    "\n",
    "-   `--typical N`: 매개변수 p를 사용하여 로컬 일반 샘플링을 활성화\n",
    "\n",
    "Example usage: `--typical 0.9`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7af630bc-1334-4e70-b0e1-d40e75d79ecb",
   "metadata": {},
   "source": [
    "### Mirostat Sampling\n",
    "\n",
    "-   `--mirostat N`: Mirostat 샘플링을 활성화하여 텍스트 생성 중 혼란을 제어 (default: 0, 0 = disabled, 1 = Mirostat, 2 = Mirostat 2.0).\n",
    "-   `--mirostat-lr N`: Mirostat 학습률, 매개변수 eta(기본값: 0.1)를 설정 (default: 0.1).\n",
    "-   `--mirostat-ent N`: Mirostat 목표 엔트로피, 매개변수 tau(기본값: 5.0)를 설정 (default: 5.0)., 값이 높을 수록 텍스트 다양성 증가\n",
    "\n",
    "Example usage: `--mirostat 2 --mirostat-lr 0.05 --mirostat-ent 3.0`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b6fa275-9e46-4a49-b5c1-e16607287023",
   "metadata": {},
   "source": [
    "### Logit Bias\n",
    "\n",
    "-   `-l TOKEN_ID(+/-)BIAS, --logit-bias TOKEN_ID(+/-)BIAS`: 생성된 텍스트에 특정 토큰이 나타날 가능성을 수동으로 조정\n",
    "\n",
    "\n",
    "예를 들어 `--logit-bias 15043+1`을 사용하여 'Hello' 토큰의 가능성을 높이거나 `--logit-bias 15043-1`을 사용하여 가능성을 줄입니다. 음의 무한대 값을 사용하면 `--logit-bias 15043-inf`는 `Hello` 토큰이 생성되지 않도록 보장합니다.\n",
    "\n",
    "보다 실용적인 사용 사례는 `-l 29905-inf`를 사용하여 `\\` 토큰(29905)을 음의 무한대로 설정하여 `\\code{begin}` 및 `\\code{end}`의 생성을 방지하는 것입니다. (이는 LLaMA 모델 추론에 나타나는 LaTeX 코드가 널리 퍼져 있기 때문입니다.)\n",
    "\n",
    "Example usage: `--logit-bias 29905-inf`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "134004ac-74c8-464d-a004-31046c506d2a",
   "metadata": {},
   "source": [
    "### RNG Seed\n",
    "\n",
    "-   `-s SEED, --seed SEED`: 랜덤 시드 설정 (default: -1, -1 = random seed)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00508fa2-b8b6-4f64-b5cf-07d158d3dadd",
   "metadata": {},
   "source": [
    "## Performance Tuning and Memory Options\n",
    "\n",
    "이러한 옵션은 LLaMA 모델의 성능과 메모리 사용량을 개선하는 데 도움이 됩니다. 이러한 설정을 조정하면 모델의 동작을 미세 조정하여 시스템 기능에 더 적합하고 특정 사용 사례에 대한 최적의 성능을 얻을 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c0bab8f-fba8-402a-b56c-9a942ce56f5c",
   "metadata": {},
   "source": [
    "### Number of Threads\n",
    "\n",
    "-   `-t N, --threads N`: 생성 중에 사용할 스레드 수를 설정, 최적의 성능을 위해 이 값을 시스템의 물리적 CPU 코어 수(논리적 코어 수와 반대)로 설정하는 것이 좋습니다. 올바른 스레드 수를 사용하면 성능이 크게 향상될 수 있습니다.\n",
    "-   `-tb N, --threads-batch N`: 일괄 처리 및 프롬프트 처리 중에 사용할 스레드 수를 설정합니다. 일부 시스템에서는 생성 중보다 일괄 처리 중에 더 많은 수의 스레드를 사용하는 것이 좋습니다. 지정하지 않으면 일괄 처리에 사용되는 스레드 수는 생성에 사용되는 스레드 수와 동일합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5e68b79-defa-4834-bcf4-6117f1e0c3df",
   "metadata": {},
   "source": [
    "### Mlock\n",
    "\n",
    "-   `--mlock`: 메모리 매핑 시 모델이 교체되는 것을 방지하여 모델을 메모리에 잠금."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64d60d1a-e044-4ccb-9c12-aafe91ad3604",
   "metadata": {},
   "source": [
    "### No Memory Mapping\n",
    "\n",
    "-   `--no-mmap`: 모델을 메모리 매핑하지 않습니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94a67e45-86ff-4619-8ce0-bfa95f01cdae",
   "metadata": {},
   "source": [
    "\n",
    "### NUMA support\n",
    "\n",
    "-   `--numa distribute`: 각 NUMA 노드의 코어에 동일한 비율의 스레드를 고정\n",
    "-   `--numa isolate`: 프로그램이 시작되는 NUMA 노드에 모든 스레드를 고정\n",
    "-   `--numa numactl`: numactl 유틸리티로 프로그램을 시작하여 프로그램에 전달된 CPUMAP에 스레드를 고정.이는 가장 유연한 모드이며 임의 코어 사용 패턴을 허용합니다(예: 하나의 NUMA 노드에 있는 모든 코어를 사용하고 두 번째 노드에 있는 충분한 코어만 사용하여 노드 간 메모리 버스를 포화시키는 맵).\n",
    "\n",
    "이러한 플래그는 균일하지 않은 메모리 액세스를 사용하는 일부 시스템에 도움이 되는 최적화를 시도합니다. 이는 현재 위의 전략 중 하나와 mmap에 대한 프리페치 및 미리 읽기 비활성화로 구성됩니다. 후자는 한 번에 모두가 아니라 첫 번째 액세스 시 매핑된 페이지에 오류가 발생하도록 하며 스레드를 NUMA 노드에 고정하는 것과 함께 더 많은 페이지가 사용되는 NUMA 노드에서 종료됩니다. 예를 들어 이 옵션 없이 이전에 실행했기 때문에 모델이 이미 시스템 페이지 캐시에 있는 경우 페이지 캐시를 먼저 삭제하지 않으면 효과가 거의 없습니다. 이는 시스템을 재부팅하거나 Linux에서 루트로 '/proc/sys/vm/drop_caches'에 '3'을 작성하여 수행할 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40cb4117-acdc-40ed-832e-7bfab240deb0",
   "metadata": {},
   "source": [
    "### Memory Float 32\n",
    "\n",
    "-   `--memory-f32`:  메모리 키+값에 16비트 부동 소수점 대신 32비트 부동 소수점을 사용. 사용 권장 안함. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c62b96ed-df9f-4386-b626-91345251302c",
   "metadata": {},
   "source": [
    "### Batch Size\n",
    "\n",
    "-   `-b N, --batch-size N`: 프롬프트 처리를 위한 배치 크기를 설정(default:512). 이러한 대규모 배치 크기는 BLAS를 설치하고 빌드 중에 활성화한 사용자에게 도움이 된다. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6ba11dd-89d3-477b-81aa-170100b9688e",
   "metadata": {},
   "source": [
    "### Prompt Caching\n",
    "\n",
    "-   `--prompt-cache FNAME`: 초기 프롬프트 후 모델 상태를 캐시할 파일을 지정"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a224d99-24d6-4725-9783-7592a008127b",
   "metadata": {},
   "source": [
    "### Grammars\n",
    "\n",
    "-   `--grammar GRAMMAR`, `--grammar-file FILE`:: 모델 출력을 특정 형식으로 제한하기 위해 문법(인라인 또는 파일에 정의됨)을 지정. See the [GBNF guide](../../grammars/README.md) for details on the syntax."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb113198-441d-43b7-9ba6-009b9563951e",
   "metadata": {},
   "source": [
    "### Quantization\n",
    "\n",
    "성능을 크게 향상시키고 메모리 사용량을 줄일 수 있는 4비트 양자화에 대한 자세한 내용은 llama.cpp의 기본 [README](../../README.md#prepare-data--run)를 참조하세요."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42ab3d3f-5ac1-4cd4-b4e3-440b09d55cdc",
   "metadata": {},
   "source": [
    "\n",
    "## Additional Options\n",
    "\n",
    "These options provide extra functionality and customization when running the LLaMA models:\n",
    "\n",
    "-   `-h, --help`: 도움말\n",
    "-   `--verbose-prompt`: 텍스트를 생성하기 전에 프롬프트를 인쇄\n",
    "-   `-ngl N, --n-gpu-layers N`: 적절한 지원(현재 CLBlast 또는 cuBLAS)으로 컴파일할 때 이 옵션을 사용하면 계산을 위해 일부 레이어를 GPU로 오프로드할 수 있다. \n",
    "-   `-mg i, --main-gpu i`: 여러 GPU를 사용할 때 이 옵션은 모든 GPU에 계산을 분할하는 오버헤드가 가치가 없는 작은 텐서에 사용되는 GPU를 제어\n",
    "-   `-ts SPLIT, --tensor-split SPLIT`:여러 GPU를 사용할 때 이 옵션은 모든 GPU에 걸쳐 얼마나 큰 텐서를 분할해야 하는지 제어\n",
    "-   `--lora FNAME`:LoRA(낮은 순위 적응) 어댑터를 모델에 적용\n",
    "-   `--lora-base FNAME`: LoRA 어댑터에 의해 수정된 레이어의 베이스로 사용할 선택적 모델"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
